{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## linear regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"https://raw.githubusercontent.com/saketh-26/machinelearningwithpython/master/Advertising.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop('Unnamed: 0',axis = 1,inplace = True)\n",
    "#Collecting X and Y\n",
    "X = data['TV'].values\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = data['sales'].values\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#calculating the coefficient.Mean X and Y\n",
    "mean_x = np.mean(X)\n",
    "print(mean_x)\n",
    "mean_y = np.mean(Y)\n",
    "print(mean_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Total number of values\n",
    "n = len(X)\n",
    "#using the formula to calculate b1 and b0  y = b1x + b0\n",
    "numer = 0\n",
    "denom = 0\n",
    "for i in range(n):\n",
    "    numer += (X[i]-mean_x) * (Y[i]-mean_y)\n",
    "    denom += (X[i]-mean_x) ** 2\n",
    "b1 = numer/denom\n",
    "b0 = mean_y - (b1 *mean_x)\n",
    "print(b1)\n",
    "print(b0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The linear model is :Y = {:.5} + {:.4}X\".format(b0,b1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#plotting values and regression line\n",
    "max_x = np.max(X)\n",
    "min_x = np.min(X) \n",
    "\n",
    "#calculating line values x and y\n",
    "x = np.linspace(min_x,max_x,1000)\n",
    "y = b0 + b1*x\n",
    "\n",
    "#ploting line\n",
    "plt.plot(x,y,color = 'red',label='Regressionline')\n",
    "#Plotting scatter points\n",
    "plt.scatter(X,Y,c = 'Blue',label = 'Scatter Plot')\n",
    "plt.xlabel('Money Spent on TV ads($)')\n",
    "plt.ylabel('Sales($)')\n",
    "plt.legend(loc ='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R-squared is a statistical measure of how close the data are to the fitted regression line. It is also known as the coefficient of determination, or the coefficient of multiple determination for multiple regression.\n",
    "\n",
    "R-squared is always between 0 and 100%:\n",
    "\n",
    "0% indicates that the model explains none of the variability of the response data around its mean. 100% indicates that the model explains all the variability of the response data around its mean.\n",
    "\n",
    "The Formula for R-Squared is : R^2 = 1-Explained Variation/Total Variation.\n",
    "\n",
    "R-squared values range from 0 to 1 and are commonly stated as percentages from 0% to 100%.An R-squared of 100% means that all movements of a security (or other dependent variable) are completely explained by movements in the index (or the independent variable(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_pred = b0 + b1*X[2]\n",
    "print(y_pred)\n",
    "#calcualting R^2 Score\n",
    "ss_tot = 0\n",
    "ss_res = 0\n",
    "for i in range(n):\n",
    "    y_pred = b0 + b1*X[i]\n",
    "    ss_tot += (Y[i]-mean_y) ** 2\n",
    "    ss_res += (y_pred - mean_y) ** 2\n",
    "r2 = (ss_res/ss_tot)\n",
    "print(\"R2 Score\")\n",
    "print(r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Collecting the data\n",
    "X = data['TV'].values.reshape(-1,1) #to get \n",
    "y = data['sales'].values.reshape(-1,1)\n",
    "reg = LinearRegression()\n",
    "reg.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(reg.coef_[0][0])\n",
    "print(reg.intercept_[0])\n",
    "print(\"The linear model is :Y = {:.5} + {:.4}X\".format(reg.intercept_[0],reg.coef_[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = reg.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (8,4))\n",
    "plt.scatter(data['TV'],data['sales'],c = 'black',label = 'scatterplot')\n",
    "plt.plot(data['TV'],prediction,c = 'blue',label = 'regressionline')\n",
    "plt.xlabel(\"Money Spent on TV ads($)\")\n",
    "plt.ylabel('Sales($)')\n",
    "plt.legend(loc ='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#r2 score\n",
    "r2 = reg.score(X,y)\n",
    "print(\"R2 score:\",r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.predict(X[[2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if your data is actually more complex than a simple straight line? Surprisingly you can actually use a linear model to fit non-linear data.A simple way to do this is to add powers of each feature as new features,then train a linear model on this extended set of features.This technique is called \"Polynomial Regression\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets look the following example\n",
    "m = 150\n",
    "X = 6*np.random.rand(m,1)\n",
    "y = 0.5 * X**2 + X + 2 + np.random.randn(m,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X,y,'.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Clearly,a st.line will never fit this data properly,so let's use Scikit-learn's PolynomialFeatures class to transform our training data by adding the square(2nd-degree polynomial) of each feature in the training set as new feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly_features = PolynomialFeatures(degree = 2,include_bias = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "degree:integer The degree of the polynomial features. Default = 2.\n",
    "\n",
    "include_bias:boolean If True (default), then include a bias column, the feature in which all polynomial powers are zero (i.e. a column of ones - acts as an intercept term in a linear model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_poly = poly_features.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_poly[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X_poly contains the original feature of X plus the square of this feature,now we can fit a LinearRegression model to this extended training data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lin_reg = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg.fit(X_poly,y)\n",
    "lin_reg.intercept_,lin_reg.coef_   #0.5*X**2 + X + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X,y,color = 'blue',label = 'input')\n",
    "plt.plot(X,lin_reg.predict(X_poly),'.',color = 'r',label = 'predictions')\n",
    "plt.legend(loc = 'best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Polynomial Regression model is the extension of the simple linear model by adding the extra predictors obtained by raising(squaring) each of the original predictors to a power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collecting the data\n",
    "X = data.iloc[:,0:3].values\n",
    "y = data.iloc[:,3:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin = LinearRegression()\n",
    "lin.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting the Polynomial Regression to the dataset\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly = PolynomialFeatures(degree = 4)\n",
    "X_poly = poly.fit_transform(X)\n",
    "poly.fit(X_poly,y)\n",
    "lin2 = LinearRegression()\n",
    "lin2.fit(X_poly,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lin.coef_)\n",
    "print(lin.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin.score(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = lin2.predict(poly.fit_transform([[230.1,37.8,69.2]]))\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
